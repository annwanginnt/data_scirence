{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install\n",
    "pip install scikit-learn\n",
    "pip install numpy\n",
    "pip install statsmodels\n",
    "pip install dmba\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import math\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from statsmodels.api import qqplot\n",
    "from statsmodels.formula.api import ols  #linear regression\n",
    "from statsmodels.formula.api import logit   #logistic regression\n",
    "from statsmodels.graphics.mosaicplot import mosaic # plot confusion matrix\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge, LassoCV, BayesianRidge\n",
    "import statsmodels.formula.api as sm\n",
    "import matplotlib.pylab as plt\n",
    "from pathlib import Path\n",
    "\n",
    "import dmba\n",
    "from dmba import regressionSummary, exhaustive_search\n",
    "from dmba import backward_elimination, forward_selection, stepwise_selection\n",
    "from dmba import adjusted_r2_score, AIC_score, BIC_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Load dataset\n",
    "df = sns.load_dataset('df')\n",
    "df = pd.read_csv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PANDAS\n",
    "df = df.rename(columns = {'old_name' : 'new_name'})\n",
    "df['new_name'] = df['old_name']\n",
    "df.set_index('column', inplace=True) #set certain column as index\n",
    "df.index\n",
    "df.columns\n",
    "df.axes\n",
    "df.dtypes\n",
    "df.copy()\n",
    "df.groupby('col').describe().describe() #double describe() to create a summary of a summary\n",
    "\n",
    "df.isnull().count()\n",
    "df.isna().count()\n",
    "df.isnull.sum()\n",
    "\n",
    "# check missing values by columns\n",
    "missing_value_counts = merged.isna().sum()\n",
    "print(missing_value_counts)\n",
    "\n",
    "\n",
    "\n",
    "df['date'] = pd.to_datetime['date']  convert the datatype\n",
    "\n",
    "random_products = np.random.choice(product_list, size = missing_data) ## Create an array of random choice with 91 elements\n",
    "\n",
    "df_test_3.loc[df_test_3['Product'].isnull(), 'Product'] = random_products # Identify missing data using loc to find the index of missing rows, and isolating the Product column\n",
    "# and fill in with new array\n",
    "\n",
    "\n",
    "df_outlier = df[df['col'] > 1000].index\n",
    "df.drop(99, axis=0, inplace=True)\n",
    "df.drop(df[df['col'] == 120].index, axis=0, inplace =True)\n",
    "df.drop_duplicates(inplace=True)\n",
    "df.dropna(subset=['col'], axis=0, inplace=True)\n",
    "\n",
    "df['col'] = df['col'].fillna('Other')\n",
    "df['col'] = df['col'].fillna(df['col'].mean())\n",
    "\n",
    "\n",
    "df.sort_index(axis=0, ascending=True)   # sort values based on index value\n",
    "df.sort_index(axis=1, ascending=True)\n",
    "df2 = df.set_index('customers').drop(['James']) # drop a column or row \n",
    "df.sort_values('column', ascending=False)\n",
    "df.column # obtain the valus of a specific column\n",
    "df['column'] # obtain the valus of a specific column\n",
    "df.loc[[]] # get rows or columns with particular labels from the index\n",
    "df.iloc[2:4] # get row or columns at particular position in the index\n",
    "df.loc[:1101, :'Province']\n",
    "df[(df['column'] > 'xx') | (df['column'] < 'yy')] # comparison operator\n",
    "    # use isin() to find records containing certain information or range of data points.\n",
    "df[df.index.isin[100,101]]\n",
    "df[df['column'].isin(range(2,4))]\n",
    "df[df['column'].isin(['Ann Wang', 'Allen Jia'])]\n",
    "df[(df['column'] > 2) & (df['column2'].isin(['gap','HM']))]\n",
    "    # use str() funtions to find if specific strings in entries contain certain letters\n",
    "df[df.column.str.lower().str.contains('m'),['Province']]\n",
    "df_grouped = df.groupby('column')\n",
    "df_grouped.get_group('column')\n",
    "df.groupby('column').mean()['column2']\n",
    "df.groupby('col1')['col2'].mean()\n",
    "df.groupby('column1')[['column2', 'column3']].aggregate(['mean', 'median', 'max'])\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "    # merge two df\n",
    "df2.set_index('customer_id', inplace=True)\n",
    "df_merged = pd.merge(left=df1, right = df2, left_on='customer_id')\n",
    "df_merged.median()\n",
    "df_merged.groupby('columns')['column2'].aggregate(['mean', 'median'])\n",
    "\n",
    "# 分成BINS, from continours to categorical\n",
    "df['col'] = pd.cut(df['col'], bins=[0, 5, 10, 15])\n",
    "\n",
    "# extracgting hours from a datatime variable\n",
    "bikes['datetime'] = pd.to_datetime(bikes['datetime'])\n",
    "bikes['hour'] = bikes['datetime'].dt.hour\n",
    "\n",
    "#scaling/standardizing data ( range 0-1)\n",
    "scaler = StandardScaler() # Z-score 标准化， 将特征值转换为0-1的分布，消除偏差\n",
    "df_std = scaler.fit_transform(df) #对df的数据 进行标\n",
    "# infering new data points\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions and create new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column with the bin of counts\n",
    "# step 1: create a function to classify amounts, \n",
    "def total_cat(x):\n",
    "    if x>= 0 and x<10:\n",
    "        return '0-10'\n",
    "    elif x>=10 and x<50:\n",
    "        return '10-50'\n",
    "    elif x>=50 and x<100:\n",
    "        return '50-100'\n",
    "    else:\n",
    "        return '100+'\n",
    "    \n",
    "# step 2: create a new column \n",
    "# step 3: the .apply() method is used to apply the total_cat() function to each value in the 'count' column of the bikes DataFrame.\n",
    "bikes['rental_total_group'] =bikes['count'].apply(total_cat)\n",
    "\n",
    "\n",
    "# create function with 2 inputs -temp and humidity -to classify goood/bad days\n",
    "def good_bad(temp, hum):\n",
    "    if temp > 25 and hum >70:\n",
    "        return 'too hot'\n",
    "    elif temp<=25 and hum <=70 and hum > 50:\n",
    "        return 'so so day'\n",
    "    else:\n",
    "        return 'good day'\n",
    "\n",
    "# apply function and create a new column \n",
    "bikes['day_type'] = bikes.apply(lambda x: good_bad(x['temp'], x['humidity']), axis =1)\n",
    "\n",
    "#The .apply() method is used to apply a function along an axis (either rows or columns) of the DataFrame.\n",
    "# another way to apply function\n",
    "def application_function(x):\n",
    "    good_bad(x['temp'], x['humidity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # measure central tendency\n",
    "np.mean(data)\n",
    "np.median(data)\n",
    "np.max(data)\n",
    "np.min(data)\n",
    "np.sqrt()\n",
    "np.arrange(0,21)\n",
    " \n",
    "stats.mode(data)[0][0]\n",
    "\n",
    "# Measure of Dispersion\n",
    "\n",
    "np.ptp(data)    #range peak to peak\n",
    "np.var(data)    # variance\n",
    "np.std(data)    # standard deviation\n",
    "scipy.stats.iqr(data) #iqr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skewness & Kurtosis\n",
    "\n",
    "scipy.stats.skew(data)   or stats.skew()\n",
    "scipy.stats.kurtosis(data, fisher=True)\n",
    "stats.zscore(data)  #z-score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measuring relationship\n",
    "pearson_corr, p_val = stats.pearsonr()  # two linear variables\n",
    "df.corr(numeric_only=True)  # calculate Pearson's correlation directly in Pandas\n",
    "np.cov(x,y)[0,1]    # covariance\n",
    "corr, p_val = stats.spearsmanr(x,y) # x, y 为数列 # Spearsman\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOTS\n",
    "\n",
    "fig = plt.figure()  # create a new figure, including multiple plots in one figure\n",
    "sns.histplot(df['col']) \n",
    "sns.scatterplot()   # two numerical variables\n",
    "sns.regplot()   \n",
    "sns.resiplot() # residual plot\n",
    "sns.lineplot()\n",
    "sns.countplot() # 创建一个条形图， 每个条形图对应数据库中的一个类别('cat''dog','rabbit')\n",
    "\n",
    "sns.lmplot（ x,y, hue）  #加上分类和颜色时  #hue for categories  # for linear regression plto\n",
    "sns.boxenplot(  showmeans=True)   # categorical \n",
    "sns.boxplot(x=df['airline'],y=df['price'],palette='hls')  # mutictegories on X-axis. \n",
    "df.plot(kind=bar)   #categorical variable vs numerical variable. \n",
    "df.hist(figsize=(12, 10)) # we can quickly create all histograms at once \n",
    "corr_matrix = titanic.corr()\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')   #heatmap\n",
    "plt.axline(xy1=(150,150), slope=1, linewidth=2, color ='green') #father_son data\n",
    "plt.axline(y=1, linestyle='dotted')\n",
    "sns.residplot()     #plot residual\n",
    "plt.title()\n",
    "plt.ylabel\n",
    "plt.xlabel\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(1.3,1))  # put legend out of the box(not cover anything)\n",
    "plt.figure(figsize=(10,5))\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "plt.axis('equal')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fillin, drop & outlier\n",
    "df['age'].fillna(df['age'].median(), inplace=True) # missing value\n",
    "df.drop_duplicates(inplace=True) # drop duplicated\n",
    "df.duplicated()\n",
    "df['sex'] = df['sex'].astype('category') # convert datatype\n",
    "\n",
    "# Addressing outliers: remove rows where 'fare' is greater than 3 standard deviations from the mean\n",
    "fare_mean = df['fare'].mean()\n",
    "fare_std = df['fare'].std()\n",
    "df = df[(df['fare'] >= fare_mean - 3 * fare_std) & (titanic['fare'] <= fare_mean + 3 * fare_std)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis tesing\n",
    "\n",
    "t_stat, p_val =stats.ttest_ind(df1, df2)    # only two groups\n",
    "print(f'T-statistic: {t_stat}')\n",
    "print(f'P-value: {p_val}')\n",
    "    #chi-squared test, only for categorical \n",
    "ris['sepal_width_cat'] = pd.cut(iris['sepal_width'], bins=[0, 3, 3.5,]\n",
    "contingency_table = pd.crosstab(iris['species'], iris['sepal_width_cat'])\n",
    "chi2, p_val, dof, expected = stats.chi2_contingency(contingency_table)\n",
    "\n",
    "f_stat, p_val = stats.f_oneway(df1,df2,df3) #ANOVA test\n",
    "pearson_corr, p_val =stats.pearsonr(df[col1], df['col2'])   # between two columns-variables\n",
    "df.corr(numeric_only=True)  # calculate Pearson's correlation directly in pandas\n",
    "U_stat, p_val = stats.mannwhitneyu(df1,df2) # non-parametric test, Mann-Whitney compare means\n",
    "H, pval = stats.kruskal(df1, df2, df3)\n",
    "print('The Test statistic: ', H)\n",
    "print('The p_value of the test: ', pval) # 注意，这里print时与其它 不一样\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimenntation\n",
    "\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "Step1- terationDataclearning\n",
    "\n",
    "step2 - users are similar in behaviour = no significant differences between variablws.\n",
    "use statistic test to support our analysis \n",
    "p_val = stats.f_oneway(df1,df2,df3) # use ANONA test  betwen 3 groups by variables\n",
    "\n",
    "step3 - Experiment Design\n",
    "select subsets of users\n",
    "    np.sample()\n",
    "    or variant1 = df.iloc[0：300]\n",
    "        variant2 = df.iloc[300:600]\n",
    "        control =df.iloc[600:]\n",
    "    create new column to add the variant identifier\n",
    "        variant1['variant'] = 'variant1'\n",
    "        variant2['variant'] = 'variant2'\n",
    "        control['variant'] = 'control'\n",
    "    concatenate all 3 data sets\n",
    "        final_dataset = pd.concat([variant1, variant2, control], axis=0)\n",
    "        \n",
    "step4: Analyzing experiment results---ANOVA test\n",
    "    var1 = results[results['Variant']=='Variant 1']['Post_Test_Avg_Spend']\n",
    "    var2 = results[results['Variant']=='Variant 2']['Post_Test_Avg_Spend']\n",
    "    control = results[results['Variant']=='Control']['Post_Test_Avg_Spend']\n",
    "\n",
    "    p_val = stats.f_oneway(var1, var2, control)\n",
    "    print(p_val)\n",
    "    \n",
    "step4: Analyzing experiment results---Independent t-test for each variant \n",
    "     var2 = results[results['Variant']=='Variant 2']['Post_Test_Avg_Spend']\n",
    "    control = results[results['Variant']=='Control']['Post_Test_Avg_Spend']\n",
    "\n",
    "    p_val = stats.ttest_ind(var1, control)\n",
    "    p_val_2 = stats.ttest_ind(var2, control)\n",
    "    p_val_3 = stats.ttest_ind(var1, var2)\n",
    "    print(p_val)\n",
    "    print(p_val_2)\n",
    "    print(p_val_3)\n",
    "    \n",
    "step 5: Now that we know the discount worked well in improving spend, let's see which variant worked better.\n",
    "    results.groupby('Variant')[['Post_Test_Avg_Play_Time']].mean()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# standardize the feature 缩放器\n",
    "scaler = StandardScaler() # Z-score 标准化， 将特征值转换为0-1的分布，消除偏差\n",
    "df_std = scaler.fit_transform(df) #对df的数据 进行标准化处理\n",
    "       #  or scaler.fit(df)\n",
    "# Perform PCA\n",
    "pca = PCA() #    n_components默认为none, 表示保留所有的主成分\n",
    "df_pca = pca.fit_transform(df_std) # 转换后的数据与原始数据相同的维度\n",
    "\n",
    "\n",
    "# The transformed data is an array, convert it back to a dataframe\n",
    "df_pca = pd.DataFrame(df_pca, columns=[f'PC{i+1}' for i in range(len(df.columns))])\n",
    "\n",
    "# Print the explained variance ratio\n",
    "print('Explained variance: ', pca.explained_variance_ratio_)\n",
    "\n",
    "# print the cumulative explained variance ratio\n",
    "cumsum_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "print('Cumulative explained variance ratio: ', cumsum_variance)\n",
    "\n",
    "#show the first few rows of transfomed dataframe\n",
    "df_pca.head()\n",
    "\n",
    "\n",
    "# let's do the same, but now reduce to 2 components \n",
    "# perform PCA\n",
    "pca = PCA(n_components=2)\n",
    "df_pca = pca.fit_transform(df_std)\n",
    "\n",
    "# The transormed data is an array, convert it back into a dataframe \n",
    "df_pca = pd.DataFrame(df_pca)\n",
    "\n",
    "# Print the explained variance ration\n",
    "print('Explained variance ratio: ', pca.explained_variance_ratio_)\n",
    "\n",
    "# Print the cumulative explained variance ratio \n",
    "cumsum_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "print('Cumulative explained variance ratio: ', cumsum_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature engineering -- convert dummie variables\n",
    "\n",
    "\n",
    "# Transform datetime to a datetime data format\n",
    "df2['datetime'] = pd.to_datetime(df2['datetime'])\n",
    "df2['hour'] = df2['datetime'].dt.hour\n",
    "df2['month'] = df2['datetime'].dt.month\n",
    "df2['day'] = df2['datetime'].dt.day\n",
    "\n",
    "# create funciton to classify amounts \n",
    "def total_cat(x):\n",
    "    if x >= 0 and x < 10:\n",
    "        return '0-10'\n",
    "    elif x >= 50 and x <100:\n",
    "        return '50-100'\n",
    "    else:\n",
    "        return '100'\n",
    "    \n",
    "# create new column with rental count range using total_cat()function\n",
    "df2['rental_total_group'] = df2['count'].apply(total_cat)\n",
    "\n",
    "# create function with 2 inputs -temp and humidity to classify \n",
    "def good_bad(temp,hum):\n",
    "    if temp > 24 and hum >70:\n",
    "        return 'too hot'\n",
    "    elif temp <=25 and hum >=50 and hum <=70:\n",
    "        return 'so so day'\n",
    "    else:\n",
    "        return 'good day'\n",
    "    \n",
    "# apply function\n",
    "df2['day_type'] = df2.apply(lambda x: good_bad(x['temp'], x['humidity']), axis=1)\n",
    "\n",
    "# Dummy variables -convert season to dummies\n",
    "# first, rename season\n",
    "season_mapping ={1:'winter', 2:'spring', 3:'summer', 4:'fall'}\n",
    "df2['season'] = df2['season'].map(season_mapping)\n",
    "\n",
    "# Create season dummies\n",
    "season_dummies = pd.get_dummies(df2['season'])\n",
    "season_dummies.head()\n",
    "\n",
    "df2 = pd.concat([df2,season_dummies], axis =1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression \n",
    "   \n",
    "    Run a model\n",
    "    fit the model\n",
    "    visualize the model\n",
    "    measurment / performance\n",
    "    outlier/leverage/influence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.formula.api import ols \n",
    "model = ols('y~x', data=df)\n",
    "price = ols('y!x', data=df).fit()\n",
    "coeffs = model.params   # get the coefficients \n",
    "\n",
    "intercept = model.params[0] # get the intercept\n",
    "slope = model.params[1] # get the sloep\n",
    "model.summary() # get OLS report\n",
    "\n",
    "rse = np.sqrt(mse)\n",
    "mse = model.mse_resid            #squared residual(squared standard error)\n",
    "rse = coeff_determination = df['x'].corr(df['y']) # R_square of X and Y\n",
    "rse = coeff_determination = model.rsquared    #R_square\n",
    "\n",
    "summary = model.get_influence().summary_frame() # leverage, influence & other metrics\n",
    "#leverage: how extreme the data point is, the bigger, the more extreme\n",
    "#influence based on the size of the residuals and leverage\n",
    "\n",
    "\n",
    "\n",
    "#create explanatory_data\n",
    "explanatory_data=pd.DataFrame({'n_convenience': np.arrange(0,11)})\n",
    "#use model to predict with explanatory_data\n",
    "price = model.predict(explanatory_data)\n",
    "# fitted value, prediciton on the original dataset\n",
    "model.fittedvalues\n",
    "\n",
    "# create 'prediction_data' column\n",
    "prediction_data = explanatory_data.assign(price = model.predict(explanatory_data))\n",
    "\n",
    "# visualize\n",
    "fig = plt.figure()\n",
    "sns.regplot(x='a', y='b', data=df, ci=None)\n",
    "sns.scatterplot(x='a', y='b', data=predication_data, color='red', marker='s')\n",
    "plt.show() \n",
    "plt.axline(xy1=(150,150), slope=1, linewidth =2, color='green') # father-son data\n",
    "\n",
    "sns.residplot() #residual plot\n",
    "qqplot(data=model.resid, fit=True, line='45') # compare the data quantiles to a normal distribution\n",
    "\n",
    "\n",
    "# \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parallel slope linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create series P as all combination of values of two variables. \n",
    "p= product(n_convenience, house_age_years) \n",
    "\n",
    "# transofrm p to a DataFrame and name the columns\n",
    "explanatory_data = pd.DataFrame(p, columns= ['n_convenience', 'house_age_years'])\n",
    "\n",
    "#add prediciton column to the DataFrame\n",
    "prediction_data = explanatory_data.assign(price = model.predict(explanatory_data))\n",
    "\n",
    "# \n",
    "coeffs = model.params\n",
    "ic_0_15 = coeffs[0]\n",
    "ic_15_30 = coeffs[1]\n",
    "ic_30_45 = coeffs[2]\n",
    "\n",
    "plt.axline(xy1=(0,ic_0_15), slope=slope, color ='green')\n",
    "plt.axline(xy1=(0,ic_15_30), slope=slope, color ='red')\n",
    "plt.axline(xy1=(0,ic_30_45), slope=slope, color ='blue')\n",
    "\n",
    "sns.scatterplot(x='n_convenience', y='price', hue='house_age_years', data=df)\n",
    "#add the prediction in black\n",
    "sns.scatterplot(x='n_convenience', y='price', color='black', data=prediction_data)\n",
    "\n",
    "#mannually predict\n",
    "# conditions, choices, np.select , then create prediciton_data\n",
    "conditions \n",
    "choices\n",
    "intercept = np.select(conditions, choices)\n",
    "#create prediciton data with columns intercept and price\n",
    "prediction_data = explanatory_data.assign(intercept=intercept, price = intercept + slope * explanatory_data['n_convenience'])\n",
    "\n",
    "\n",
    "# measurement\n",
    "\n",
    "model.rsquared # larger the better #r_squared = coefficient of determination \n",
    "model.rsqared_adj   #adjusted_r_square\n",
    "model.mse_resid # MSE\n",
    "np.sqrt(model.mse_resid) #RSE = residual standard error #smaller is better\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Transformation\n",
    "- Square roots: when the data has a right skewed distribution, 低点太紧凑"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.formula.api import logit\n",
    "from statsmodels.graphics.mosaicplot import mosaic\n",
    "\n",
    "model = logit('y~x', data=df).fit()\n",
    "model.params # get two coefficients, intercept and slope\n",
    "\n",
    "#visualizing\n",
    "sns.regplot(x='a', y='b', data=df,ci=None, logistic=True)\n",
    "plt.axline(xy1(0,intercept), slope=slope, color='red')\n",
    "\n",
    "#probability, mostlikely outcome, oddsratio, log oddsratio\n",
    "# get the most likely outcome 0 or 1\n",
    "prediction_data['most_likely_outcome'] = np.round(prediction_data['has_churned'])\n",
    "odds_ratio = probability/1-probability\n",
    "prediction_data['log_odds_ratio'] = np.log(prediction_dta['odds_ratio'])\n",
    "\n",
    "#measurement\n",
    "conf_matrix = model.pred_table()\n",
    "mosaic(conf_matrix) # plot confusion matrix\n",
    "\n",
    "#accuracy: the % of the correct predictions\n",
    "# sensitivity: the % of the true positive , higher is better \n",
    "# specificity: the % of true negative, higher is better\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
