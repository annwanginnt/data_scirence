{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install scikit-learn\n",
    "pip install numpy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import math\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Load dataset\n",
    "df = sns.load_dataset('df')\n",
    "df = pd.read_csv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PANDAS\n",
    "df = df.rename(columns = {'old_name' : 'new_name'})\n",
    "df['new_name'] = df['old_name']\n",
    "df.set_index('column', inplace=True) #set certain column as index\n",
    "df.index\n",
    "df.columns\n",
    "df.axes\n",
    "df.dtypes\n",
    "df.copy()\n",
    "df.groupby('col').describe().describe() #double describe() to create a summary of a summary\n",
    "\n",
    "df.isnull().count()\n",
    "df.isna().count()\n",
    "df.isnull.sum()\n",
    "\n",
    "\n",
    "\n",
    "df['date'] = pd.to_datetime['date']  convert the datatype\n",
    "\n",
    "random_products = np.random.choice(product_list, size = missing_data) ## Create an array of random choice with 91 elements\n",
    "\n",
    "df_test_3.loc[df_test_3['Product'].isnull(), 'Product'] = random_products # Identify missing data using loc to find the index of missing rows, and isolating the Product column\n",
    "# and fill in with new array\n",
    "\n",
    "\n",
    "df_outlier = df[df['col'] > 1000].index\n",
    "df.drop(99, axis=0, inplace=True)\n",
    "df.drop(df[df['col'] == 120].index, axis=0, inplace =True)\n",
    "df.drop_duplicates(inplace=True)\n",
    "df.dropna(subset=['col'], axis=0, inplace=True)\n",
    "\n",
    "df['col'] = df['col'].fillna('Other')\n",
    "df['col'] = df['col'].fillna(df['col'].mean())\n",
    "\n",
    "\n",
    "df.sort_index(axis=0, ascending=True)   # sort values based on index value\n",
    "df.sort_index(axis=1, ascending=True)\n",
    "df2 = df.set_index('customers').drop(['James']) # drop a column or row \n",
    "df.sort_values('column', ascending=False)\n",
    "df.column # obtain the valus of a specific column\n",
    "df['column'] # obtain the valus of a specific column\n",
    "df.loc[[]] # get rows or columns with particular labels from the index\n",
    "df.iloc[2:4] # get row or columns at particular position in the index\n",
    "df.loc[:1101, :'Province']\n",
    "df[(df['column'] > 'xx') | (df['column'] < 'yy')] # comparison operator\n",
    "    # use isin() to find records containing certain information or range of data points.\n",
    "df[df.index.isin[100,101]]\n",
    "df[df['column'].isin(range(2,4))]\n",
    "df[df['column'].isin(['Ann Wang', 'Allen Jia'])]\n",
    "df[(df['column'] > 2) & (df['column2'].isin(['gap','HM']))]\n",
    "    # use str() funtions to find if specific strings in entries contain certain letters\n",
    "df[df.column.str.lower().str.contains('m'),['Province']]\n",
    "df_grouped = df.groupby('column')\n",
    "df_grouped.get_group('column')\n",
    "df.groupby('column').mean()['column2']\n",
    "df.groupby('col1')['col2'].mean()\n",
    "df.groupby('column1')[['column2', 'column3']].aggregate(['mean', 'median', 'max'])\n",
    "\n",
    "\n",
    "    # merge two df\n",
    "df2.set_index('customer_id', inplace=True)\n",
    "df_merged = pd.merge(left=df1, right = df2, left_on='customer_id')\n",
    "df_merged.median()\n",
    "df_merged.groupby('columns')['column2'].aggregate(['mean', 'median'])\n",
    "\n",
    "\n",
    "df['col'] = pd.cut(df['col'], bins=[0, 5, 10, 15])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # measure central tendency\n",
    "np.mean(data)\n",
    "np.median(data)\n",
    "np.max(data)\n",
    "np.min(data)\n",
    "stats.mode(data)[0][0]\n",
    "\n",
    "# Measure of Dispersion\n",
    "\n",
    "np.ptp(data)    #range peak to peak\n",
    "np.var(data)    # variance\n",
    "np.std(data)    # standard deviation\n",
    "scipy.stats.iqr(data) #iqr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skewness & Kurtosis\n",
    "\n",
    "scipy.stats.skew(data)   or stats.skew()\n",
    "scipy.stats.kurtosis(data, fisher=True)\n",
    "stats.zscore(data)  #z-score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# measuring relationship\n",
    "pearson_corr, p_val = stats.pearsonr()  # two linear variables\n",
    "df.corr(numeric_only=True)  # calculate Pearson's correlation directly in Pandas\n",
    "np.cov(x,y)[0,1]    # covariance\n",
    "corr, p_val = stats.spearsmanr(x,y) # x, y 为数列 # Spearsman\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOTS\n",
    "\n",
    "sns.histplot(df['col'])\n",
    "sns.scatterplot\n",
    "sns.regplot()\n",
    "sns.lmplot（ x,y, hue）  #加上分类和颜色时\n",
    "sns.boxenplot\n",
    "df.hist(figsize=(12, 10)) # we can quickly create all histograms at once \n",
    "corr_matrix = titanic.corr()\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')   #heatmap\n",
    "\n",
    "plt.title()\n",
    "plt.ylabel\n",
    "plt.xlabel\n",
    "figsize(12,10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['age'].fillna(df['age'].median(), inplace=True) # missing value\n",
    "df.drop_duplicates(inplace=True) # drop duplicated\n",
    "df.duplicated()\n",
    "df['sex'] = df['sex'].astype('category') # convert datatype\n",
    "\n",
    "# Addressing outliers: remove rows where 'fare' is greater than 3 standard deviations from the mean\n",
    "fare_mean = df['fare'].mean()\n",
    "fare_std = df['fare'].std()\n",
    "df = df[(df['fare'] >= fare_mean - 3 * fare_std) & (titanic['fare'] <= fare_mean + 3 * fare_std)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypothesis test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis tesing\n",
    "\n",
    "t_stat, p_val =stats.ttest_ind(df1, df2)    # only two groups\n",
    "print(f'T-statistic: {t_stat}')\n",
    "print(f'P-value: {p_val}')\n",
    "    #chi-squared test, only for categorical \n",
    "ris['sepal_width_cat'] = pd.cut(iris['sepal_width'], bins=[0, 3, 3.5,]\n",
    "contingency_table = pd.crosstab(iris['species'], iris['sepal_width_cat'])\n",
    "chi2, p_val, dof, expected = stats.chi2_contingency(contingency_table)\n",
    "\n",
    "f_stat, p_val = stats.f_oneway(df1,df2,df3) #ANOVA test\n",
    "pearson_corr, p_val =stats.pearsonr(df[col1], df['col2'])   # between two columns-variables\n",
    "df.corr(numeric_only=True)  # calculate Pearson's correlation directly in pandas\n",
    "U_stat, p_val = stats.mannwhitneyu(df1,df2) # non-parametric test, Mann-Whitney compare means\n",
    "H, pval = stats.kruskal(df1, df2, df3)\n",
    "print('The Test statistic: ', H)\n",
    "print('The p_value of the test: ', pval) # 注意，这里print时与其它 不一样\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
