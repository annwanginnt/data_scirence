{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install\n",
    "pip install scikit-learn\n",
    "pip install numpy\n",
    "pip install statsmodels\n",
    "pip install dmba\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy import stats\n",
    "import math # ODD, Probability\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import dmba\n",
    "from dmba import regressionSummary, exhaustive_search\n",
    "from dmba import backward_elimination, forward_selection, stepwise_selection\n",
    "from dmba import adjusted_r2_score, AIC_score, BIC_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statsmodel\n",
    "\n",
    " is a Python module that provides classes and functions for the estimation of many different statistical models, as well as for conducting statistical tests and exploring data.\n",
    "\n",
    " While scikit-learn is geared more towards predictive modeling and machine learning, statsmodels focuses more on statistical modeling and hypothesis testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.api import qqplot\n",
    "from statsmodels.formula.api import ols  #linear regression\n",
    "from statsmodels.formula.api import logit   #logistic regression\n",
    "from statsmodels.graphics.mosaicplot import mosaic # plot confusion matrix\n",
    "import statsmodels.formula.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn\n",
    "\n",
    "Scikit-learn is one of the most popular machine learning libraries in Python, and it provides a wide range of tools for building machine learning models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "# apply different preprocessing steps to subsets of the column\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, ElasticNet, Ridge, Lasso,LassoCV, BayesianRidge\n",
    "# provides basic strategies for imputing (or filling in) missing values. \n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Load dataset\n",
    "df = sns.load_dataset('df')\n",
    "df = pd.read_csv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PANDAS\n",
    "df = df.rename(columns = {'old_name' : 'new_name'})\n",
    "df['new_name'] = df['old_name']\n",
    "df.set_index('column', inplace=True) #set certain column as index\n",
    "df.index\n",
    "df.columns\n",
    "df.axes\n",
    "df.dtypes\n",
    "df.copy()\n",
    "df.groupby('col').describe().describe() #double describe() to create a summary of a summary\n",
    "\n",
    "df.isnull().count()\n",
    "df.isna().count()\n",
    "df.isnull.sum()\n",
    "\n",
    "# check missing values by columns\n",
    "missing_value_counts = merged.isna().sum()\n",
    "print(missing_value_counts)\n",
    "\n",
    "\n",
    "# fillin, drop & outlier\n",
    "df['age'].fillna(df['age'].median(), inplace=True) # missing value\n",
    "df.drop_duplicates(inplace=True) # drop duplicated\n",
    "df.duplicated()\n",
    "df['sex'] = df['sex'].astype('category') # convert datatype\n",
    "\n",
    "# Addressing outliers: remove rows where 'fare' is greater than 3 standard deviations from the mean\n",
    "fare_mean = df['fare'].mean()\n",
    "fare_std = df['fare'].std()\n",
    "df = df[(df['fare'] >= fare_mean - 3 * fare_std) & (titanic['fare'] <= fare_mean + 3 * fare_std)]\n",
    "     \n",
    "\n",
    "\n",
    "df['date'] = pd.to_datetime['date']  convert the datatype\n",
    "\n",
    "random_products = np.random.choice(product_list, size = missing_data) ## Create an array of random choice with 91 elements\n",
    "\n",
    "df_test_3.loc[df_test_3['Product'].isnull(), 'Product'] = random_products # Identify missing data using loc to find the index of missing rows, and isolating the Product column\n",
    "# and fill in with new array\n",
    "\n",
    "\n",
    "df_outlier = df[df['col'] > 1000].index\n",
    "df.drop(99, axis=0, inplace=True)\n",
    "df.drop(df[df['col'] == 120].index, axis=0, inplace =True)\n",
    "df.drop_duplicates(inplace=True)\n",
    "df.dropna(subset=['col'], axis=0, inplace=True)\n",
    "\n",
    "df['col'] = df['col'].fillna('Other')\n",
    "df['col'] = df['col'].fillna(df['col'].mean())\n",
    "\n",
    "\n",
    "df.sort_index(axis=0, ascending=True)   # sort values based on index value\n",
    "df.sort_index(axis=1, ascending=True)\n",
    "df2 = df.set_index('customers').drop(['James']) # drop a column or row \n",
    "df.sort_values('column', ascending=False)\n",
    "df.column # obtain the valus of a specific column\n",
    "df['column'] # obtain the valus of a specific column\n",
    "df.loc[[]] # get rows or columns with particular labels from the index\n",
    "df.iloc[2:4] # get row or columns at particular position in the index\n",
    "df.loc[:1101, :'Province']\n",
    "df[(df['column'] > 'xx') | (df['column'] < 'yy')] # comparison operator\n",
    "    # use isin() to find records containing certain information or range of data points.\n",
    "df[df.index.isin[100,101]]\n",
    "df[df['column'].isin(range(2,4))]\n",
    "df[df['column'].isin(['Ann Wang', 'Allen Jia'])]\n",
    "df[(df['column'] > 2) & (df['column2'].isin(['gap','HM']))]\n",
    "    # use str() funtions to find if specific strings in entries contain certain letters\n",
    "df[df.column.str.lower().str.contains('m'),['Province']]\n",
    "df_grouped = df.groupby('column')\n",
    "df_grouped.get_group('column')\n",
    "df.groupby('column').mean()['column2']\n",
    "df.groupby('col1')['col2'].mean()\n",
    "df.groupby('column1')[['column2', 'column3']].aggregate(['mean', 'median', 'max'])\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "    # merge two df\n",
    "df2.set_index('customer_id', inplace=True)\n",
    "df_merged = pd.merge(left=df1, right = df2, left_on='customer_id')\n",
    "df_merged.median()\n",
    "df_merged.groupby('columns')['column2'].aggregate(['mean', 'median'])\n",
    "\n",
    "# 分成BINS, from continours to categorical\n",
    "df['col'] = pd.cut(df['col'], bins=[0, 5, 10, 15])\n",
    "\n",
    "# extracgting hours from a datatime variable\n",
    "bikes['datetime'] = pd.to_datetime(bikes['datetime'])\n",
    "bikes['hour'] = bikes['datetime'].dt.hour\n",
    "\n",
    "#scaling/standardizing data ( range 0-1)\n",
    "scaler = StandardScaler() # Z-score 标准化， 将特征值转换为0-1的分布，消除偏差\n",
    "df_std = scaler.fit_transform(df) #对df的数据 进行标\n",
    "# infering new data points\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PLOTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot() #lineplot\n",
    "plt.scatter()  # scatter plot\n",
    "plt.bar()\n",
    "plt.hist()\n",
    "plt.boxplot()\n",
    "plt.pie()\n",
    "plt.imshow() # display data as an image , for example in heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.histplot(df['col']) \n",
    "sns.scatterplot()   # two numerical variables\n",
    "sns.regplot()  # scatter plot with a regression line.\n",
    "sns.lmplot() # scatter plot with a regression line.\n",
    "sns.barplot()\n",
    "sns.violinplot() # combinatin of a boxplot and kernel density estimate  \n",
    "sns.resiplot() # residual plot\n",
    "sns.lineplot()\n",
    "sns.pairplot() # showing pairwise relationships in a dataset.\n",
    "sns.countplot() # 创建一个条形图， 每个条形图对应数据库中的一个类别('cat''dog','rabbit')\n",
    "sns.residplot()     #plot residual\n",
    "sns.lmplot（ x,y, hue）  #加上分类和颜色时  #hue for categories  # for linear regression plto\n",
    "sns.boxenplot(  showmeans=True)   # categorical \n",
    "sns.boxplot(x=df['airline'],y=df['price'],palette='hls')  # mutictegories on X-axis. \n",
    "corr_matrix = titanic.corr()\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')   #heatmap\n",
    "\n",
    "\n",
    "df.plot(kind=bar)   #categorical variable vs numerical variable. \n",
    "df.hist(figsize=(12, 10)) # we can quickly create all histograms at once \n",
    "\n",
    "#create a new figure, you can add subplots in it. \n",
    "fig = plt.figure()  \n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "# create multiple subplots in one fig.\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "fig, ax1, ax2, ax3 = plt.subplots(3, figsize=(10,8))\n",
    "fig, ((ax1, ax2), (ax3,ax4)) = plt.subplots(2,2, figsize=(10,8))\n",
    "\n",
    "plt.axline(xy1=(150,150), slope=1, linewidth=2, color ='green') #father_son data\n",
    "plt.axline(y=1, linestyle='dotted')\n",
    "\n",
    "plt.title()\n",
    "plt.ylabel\n",
    "plt.xlabel\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(1.3,1))  # put legend out of the box(not cover anything)\n",
    "\n",
    "plt.axis('equal')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # measure central tendency\n",
    "np.mean(data)\n",
    "np.median(data)\n",
    "np.max(data)\n",
    "np.min(data)\n",
    "np.sqrt()\n",
    "np.arrange(0,21)\n",
    " \n",
    "stats.mode(data)[0][0]\n",
    "\n",
    "# Measure of Dispersion\n",
    "\n",
    "np.ptp(data)    #range peak to peak\n",
    "np.var(data)    # variance\n",
    "np.std(data)    # standard deviation\n",
    "scipy.stats.iqr(data) #iqr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skewness & Kurtosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skewness & Kurtosis\n",
    "\n",
    "scipy.stats.skew(data)   or stats.skew()\n",
    "scipy.stats.kurtosis(data, fisher=True)\n",
    "stats.zscore(data)  #z-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measuring correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# measuring relationship\n",
    "pearson_corr, p_val = stats.pearsonr()  # two linear variables\n",
    "df.corr(numeric_only=True)  # calculate Pearson's correlation directly in Pandas\n",
    "np.cov(x,y)[0,1]    # covariance\n",
    "corr, p_val = stats.spearsmanr(x,y) # x, y 为数列 # Spearsman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions and create new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column with the bin of counts\n",
    "# step 1: create a function to classify amounts, \n",
    "def total_cat(x):\n",
    "    if x>= 0 and x<10:\n",
    "        return '0-10'\n",
    "    elif x>=10 and x<50:\n",
    "        return '10-50'\n",
    "    elif x>=50 and x<100:\n",
    "        return '50-100'\n",
    "    else:\n",
    "        return '100+'\n",
    "    \n",
    "# step 2: create a new column \n",
    "# step 3: the .apply() method is used to apply the total_cat() function to each value in the 'count' column of the bikes DataFrame.\n",
    "bikes['rental_total_group'] =bikes['count'].apply(total_cat)\n",
    "\n",
    "\n",
    "# create function with 2 inputs -temp and humidity -to classify goood/bad days\n",
    "def good_bad(temp, hum):\n",
    "    if temp > 25 and hum >70:\n",
    "        return 'too hot'\n",
    "    elif temp<=25 and hum <=70 and hum > 50:\n",
    "        return 'so so day'\n",
    "    else:\n",
    "        return 'good day'\n",
    "\n",
    "# apply function and create a new column \n",
    "bikes['day_type'] = bikes.apply(lambda x: good_bad(x['temp'], x['humidity']), axis =1)\n",
    "\n",
    "#The .apply() method is used to apply a function along an axis (either rows or columns) of the DataFrame.\n",
    "# another way to apply function\n",
    "def application_function(x):\n",
    "    good_bad(x['temp'], x['humidity'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dummy variables\n",
    "# convert season to dummies\n",
    "# rename seasons\n",
    "season_mapping = {1:'winter', 2:'spring',3:'summer',4:'fall'}\n",
    "bikes['season'] = bikes['season'].map(season_mapping)\n",
    "\n",
    "#create season dummies\n",
    "season_dummies = pd.get_dummies(bikes['season'])\n",
    "\n",
    "season_dummies = season_dummies.astype(int)\n",
    "dummies_season = pd.get_dummies(bikes['season'], dtype=int)\n",
    "bikes = pd.concat([bikes, season_dummies], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypothesis testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis tesing\n",
    "\n",
    "t_stat, p_val =stats.ttest_ind(df1, df2)    # only two groups\n",
    "print(f'T-statistic: {t_stat}')\n",
    "print(f'P-value: {p_val}')\n",
    "    #chi-squared test, only for categorical \n",
    "ris['sepal_width_cat'] = pd.cut(iris['sepal_width'], bins=[0, 3, 3.5,]\n",
    "contingency_table = pd.crosstab(iris['species'], iris['sepal_width_cat'])\n",
    "chi2, p_val, dof, expected = stats.chi2_contingency(contingency_table)\n",
    "\n",
    "f_stat, p_val = stats.f_oneway(df1,df2,df3) #ANOVA test\n",
    "pearson_corr, p_val =stats.pearsonr(df[col1], df['col2'])   # between two columns-variables\n",
    "df.corr(numeric_only=True)  # calculate Pearson's correlation directly in pandas\n",
    "U_stat, p_val = stats.mannwhitneyu(df1,df2) # non-parametric test, Mann-Whitney compare means\n",
    "H, pval = stats.kruskal(df1, df2, df3)\n",
    "print('The Test statistic: ', H)\n",
    "print('The p_value of the test: ', pval) # 注意，这里print时与其它 不一样\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimenntation\n",
    "\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "Step1- terationDataclearning\n",
    "\n",
    "step2 - users are similar in behaviour = no significant differences between variablws.\n",
    "use statistic test to support our analysis \n",
    "p_val = stats.f_oneway(df1,df2,df3) # use ANONA test  betwen 3 groups by variables\n",
    "\n",
    "step3 - Experiment Design\n",
    "select subsets of users\n",
    "    np.sample()\n",
    "    or variant1 = df.iloc[0：300]\n",
    "        variant2 = df.iloc[300:600]\n",
    "        control =df.iloc[600:]\n",
    "    create new column to add the variant identifier\n",
    "        variant1['variant'] = 'variant1'\n",
    "        variant2['variant'] = 'variant2'\n",
    "        control['variant'] = 'control'\n",
    "    concatenate all 3 data sets\n",
    "        final_dataset = pd.concat([variant1, variant2, control], axis=0)\n",
    "        \n",
    "step4: Analyzing experiment results---ANOVA test\n",
    "    var1 = results[results['Variant']=='Variant 1']['Post_Test_Avg_Spend']\n",
    "    var2 = results[results['Variant']=='Variant 2']['Post_Test_Avg_Spend']\n",
    "    control = results[results['Variant']=='Control']['Post_Test_Avg_Spend']\n",
    "\n",
    "    p_val = stats.f_oneway(var1, var2, control)\n",
    "    print(p_val)\n",
    "    \n",
    "step4: Analyzing experiment results---Independent t-test for each variant \n",
    "     var2 = results[results['Variant']=='Variant 2']['Post_Test_Avg_Spend']\n",
    "    control = results[results['Variant']=='Control']['Post_Test_Avg_Spend']\n",
    "\n",
    "    p_val = stats.ttest_ind(var1, control)\n",
    "    p_val_2 = stats.ttest_ind(var2, control)\n",
    "    p_val_3 = stats.ttest_ind(var1, var2)\n",
    "    print(p_val)\n",
    "    print(p_val_2)\n",
    "    print(p_val_3)\n",
    "    \n",
    "step 5: Now that we know the discount worked well in improving spend, let's see which variant worked better.\n",
    "    results.groupby('Variant')[['Post_Test_Avg_Play_Time']].mean()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# standardize the feature 缩放器\n",
    "scaler = StandardScaler() # Z-score 标准化， 将特征值转换为0-1的分布，消除偏差\n",
    "df_std = scaler.fit_transform(df) #对df的数据 进行标准化处理\n",
    "       #  or scaler.fit(df)\n",
    "# Perform PCA\n",
    "pca = PCA() #    n_components默认为none, 表示保留所有的主成分\n",
    "df_pca = pca.fit_transform(df_std) # 转换后的数据与原始数据相同的维度\n",
    "\n",
    "\n",
    "# The transformed data is an array, convert it back to a dataframe\n",
    "df_pca = pd.DataFrame(df_pca, columns=[f'PC{i+1}' for i in range(len(df.columns))])\n",
    "\n",
    "# Print the explained variance ratio\n",
    "print('Explained variance: ', pca.explained_variance_ratio_)\n",
    "\n",
    "# print the cumulative explained variance ratio\n",
    "cumsum_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "print('Cumulative explained variance ratio: ', cumsum_variance)\n",
    "\n",
    "#show the first few rows of transfomed dataframe\n",
    "df_pca.head()\n",
    "\n",
    "\n",
    "# let's do the same, but now reduce to 2 components \n",
    "# perform PCA\n",
    "pca = PCA(n_components=2)\n",
    "df_pca = pca.fit_transform(df_std)\n",
    "\n",
    "# The transormed data is an array, convert it back into a dataframe \n",
    "df_pca = pd.DataFrame(df_pca)\n",
    "\n",
    "# Print the explained variance ration\n",
    "print('Explained variance ratio: ', pca.explained_variance_ratio_)\n",
    "\n",
    "# Print the cumulative explained variance ratio \n",
    "cumsum_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "print('Cumulative explained variance ratio: ', cumsum_variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature engineering -- convert dummie variables\n",
    "\n",
    "\n",
    "# Transform datetime to a datetime data format\n",
    "df2['datetime'] = pd.to_datetime(df2['datetime'])\n",
    "df2['hour'] = df2['datetime'].dt.hour\n",
    "df2['month'] = df2['datetime'].dt.month\n",
    "df2['day'] = df2['datetime'].dt.day\n",
    "\n",
    "# create funciton to classify amounts \n",
    "def total_cat(x):\n",
    "    if x >= 0 and x < 10:\n",
    "        return '0-10'\n",
    "    elif x >= 50 and x <100:\n",
    "        return '50-100'\n",
    "    else:\n",
    "        return '100'\n",
    "    \n",
    "# create new column with rental count range using total_cat()function\n",
    "df2['rental_total_group'] = df2['count'].apply(total_cat)\n",
    "\n",
    "# create function with 2 inputs -temp and humidity to classify \n",
    "def good_bad(temp,hum):\n",
    "    if temp > 24 and hum >70:\n",
    "        return 'too hot'\n",
    "    elif temp <=25 and hum >=50 and hum <=70:\n",
    "        return 'so so day'\n",
    "    else:\n",
    "        return 'good day'\n",
    "    \n",
    "# apply function\n",
    "df2['day_type'] = df2.apply(lambda x: good_bad(x['temp'], x['humidity']), axis=1)\n",
    "\n",
    "# Dummy variables -convert season to dummies\n",
    "# first, rename season\n",
    "season_mapping ={1:'winter', 2:'spring', 3:'summer', 4:'fall'}\n",
    "df2['season'] = df2['season'].map(season_mapping)\n",
    "\n",
    "# Create season dummies\n",
    "season_dummies = pd.get_dummies(df2['season'])\n",
    "season_dummies.head()\n",
    "\n",
    "df2 = pd.concat([df2,season_dummies], axis =1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression \n",
    "   \n",
    "    Run a model\n",
    "    fit the model\n",
    "    visualize the model\n",
    "    measurment / performance\n",
    "    outlier/leverage/influence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OLS linear regression | residual analysis example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = ols(\"casual ~ temp + humidity + workingday\", data=bikes).fit()\n",
    "print(results.summary())\n",
    "\n",
    "pred_vals = results.predict(bikes[['temp','humidity','workingday']])\n",
    "\n",
    "res = pd.concat([pred_vals.to_frame().rename(columns={0:'y_hat'}),bikes['casual']], axis=1)\n",
    "\n",
    "res['error'] = res['y_hat'] - res['casual']\n",
    "\n",
    "res['sq_error'] = res['error']**2\n",
    "\n",
    "np.sqrt(res['sq_error'].mean())\n",
    "\n",
    "plt.hist(res['error'])\n",
    "\n",
    "plt.scatter(bikes['temp'],res['error'])\n",
    "\n",
    "plt.scatter(bikes['humidity'],res['error'])\n",
    "\n",
    "#QQ PLOT\n",
    "import scipy.stats as stats\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "stats.probplot(res['error'], dist='norm', plot=ax)\n",
    "\n",
    "# Influencer \n",
    "infl = results.get_influence()\n",
    "print(infl.summary_frame())\n",
    "\n",
    "infl.summary_frame()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "fig = sm.graphics.influence_plot(results, ax=ax, criterion='cooks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.formula.api import ols \n",
    "model = ols('y~x', data=df)\n",
    "price = ols('y~x', data=df).fit()\n",
    "print(price.summary()) #get OLS report\n",
    "coeffs = model.params   # get the coefficients \n",
    "\n",
    "intercept = model.params[0] # get the intercept\n",
    "slope = model.params[1] # get the sloep\n",
    "model.summary() # get OLS report\n",
    "\n",
    "rse = np.sqrt(mse)\n",
    "mse = model.mse_resid            #squared residual(squared standard error)\n",
    "rse = coeff_determination = df['x'].corr(df['y']) # R_square of X and Y\n",
    "rse = coeff_determination = model.rsquared    #R_square\n",
    "\n",
    "summary = model.get_influence().summary_frame() # leverage, influence & other metrics\n",
    "#leverage: how extreme the data point is, the bigger, the more extreme\n",
    "#influence based on the size of the residuals and leverage\n",
    "\n",
    "\n",
    "\n",
    "#create explanatory_data\n",
    "explanatory_data=pd.DataFrame({'n_convenience': np.arrange(0,11)})\n",
    "#use model to predict with explanatory_data\n",
    "price = model.predict(explanatory_data)\n",
    "# fitted value, prediciton on the original dataset\n",
    "model.fittedvalues\n",
    "\n",
    "# create 'prediction_data' column\n",
    "prediction_data = explanatory_data.assign(price = model.predict(explanatory_data))\n",
    "\n",
    "# visualize\n",
    "fig = plt.figure()\n",
    "sns.regplot(x='a', y='b', data=df, ci=None)\n",
    "sns.scatterplot(x='a', y='b', data=predication_data, color='red', marker='s')\n",
    "plt.show() \n",
    "plt.axline(xy1=(150,150), slope=1, linewidth =2, color='green') # father-son data\n",
    "\n",
    "sns.residplot() #residual plot\n",
    "qqplot(data=model.resid, fit=True, line='45') # compare the data quantiles to a normal distribution\n",
    "\n",
    "\n",
    "#probability plot\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "stats.probplot(res['error'], dist='norm', plot=ax)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# check influencer\n",
    "infl = results.get_influence()\n",
    "print(infl.summary_frame()) \n",
    "infl.summary_frame()\n",
    "# influencer plot\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "fig = sm.graphics.influence_plot(results, ax=ax, criterion='cooks')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parallel slope linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create series P as all combination of values of two variables. \n",
    "p= product(n_convenience, house_age_years) \n",
    "\n",
    "# transofrm p to a DataFrame and name the columns\n",
    "explanatory_data = pd.DataFrame(p, columns= ['n_convenience', 'house_age_years'])\n",
    "\n",
    "#add prediciton column to the DataFrame\n",
    "prediction_data = explanatory_data.assign(price = model.predict(explanatory_data))\n",
    "\n",
    "# \n",
    "coeffs = model.params\n",
    "ic_0_15 = coeffs[0]\n",
    "ic_15_30 = coeffs[1]\n",
    "ic_30_45 = coeffs[2]\n",
    "\n",
    "plt.axline(xy1=(0,ic_0_15), slope=slope, color ='green')\n",
    "plt.axline(xy1=(0,ic_15_30), slope=slope, color ='red')\n",
    "plt.axline(xy1=(0,ic_30_45), slope=slope, color ='blue')\n",
    "\n",
    "sns.scatterplot(x='n_convenience', y='price', hue='house_age_years', data=df)\n",
    "#add the prediction in black\n",
    "sns.scatterplot(x='n_convenience', y='price', color='black', data=prediction_data)\n",
    "\n",
    "#mannually predict\n",
    "# conditions, choices, np.select , then create prediciton_data\n",
    "conditions \n",
    "choices\n",
    "intercept = np.select(conditions, choices)\n",
    "#create prediciton data with columns intercept and price\n",
    "prediction_data = explanatory_data.assign(intercept=intercept, price = intercept + slope * explanatory_data['n_convenience'])\n",
    "\n",
    "\n",
    "# measurement\n",
    "\n",
    "model.rsquared # larger the better #r_squared = coefficient of determination \n",
    "model.rsqared_adj   #adjusted_r_square\n",
    "model.mse_resid # MSE\n",
    "np.sqrt(model.mse_resid) #RSE = residual standard error #smaller is better\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Residual Analysis\n",
    "\n",
    "    1. Residual Plot\n",
    "\n",
    "            residual =pred-y_test\n",
    "\n",
    "            plt.figure(figsize =(10,6))\n",
    "            plt.scatter()\n",
    " \n",
    "    \n",
    "    2. Normality of residuals\n",
    "    \n",
    "        In a Q-Q plot, if residuals are normally distributed, the points should fall roughly along a straight line.\n",
    "\n",
    "    3. outliers and Influential points\n",
    "\n",
    "        outlier: observations with large residuals.\n",
    "\n",
    "        influencer: would significantly change the regression results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Transformation\n",
    "- Square roots: when the data has a right skewed distribution, 低点太紧凑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build  Regression model -Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 未经处理的原始数据\n",
    "\n",
    "2. seperate features and target . categorical & numberical\n",
    "\n",
    "3. split the data\n",
    "\n",
    "4. define transformer: the rules to transform data\n",
    "\n",
    "5. build preprocessor: To repares or transforms raw data into a format that is more suitable for modeling. \n",
    "\n",
    "6. Build Pipeline. \n",
    " Allows you to chain together multiple steps in a machine learning workflow. The benefit of using a Pipeline is that it ensures a consistent workflow from preprocessing to modeling. \n",
    "\n",
    "        preprecessor step\n",
    "\n",
    "        modeling step: the estimateor used is LinearRegression\n",
    "7. Fit the model\n",
    "\n",
    "8. prefict\n",
    "\n",
    "9. Measure performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前期处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate features and target\n",
    "categorical_features = ['workingday','season', 'weather']\n",
    "numerical_features = ['temp','humidity', 'windspeed']\n",
    "target = 'casual'\n",
    "\n",
    "# split data in train / test\n",
    "X_train, X_test, y_train, y_test = train_test_split(bikes[categorical_features + numerical_features]\n",
    "                                                    ,bikes[target],\n",
    "                                                    test_size = 0.3, random_state=1234)\n",
    "\n",
    "# define transformers\n",
    "cat_transformer = Pipeline(steps = [('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "num_transformer = Pipeline(steps = [('scaler', StandardScaler())])\n",
    "\n",
    "# building processor\n",
    "preprocessor = ColumnTransformer(transformers = [('cat',cat_transformer, categorical_features),\n",
    "                                                 ('num', num_transformer, numerical_features)])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline\n",
    "linear_regression = Pipeline(steps = [\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "#fit model\n",
    "linear_regression.fit(X_train, y_train)\n",
    "\n",
    "# predict\n",
    "pred = linear_regression.predict(X_test)\n",
    "\n",
    "# measure \n",
    "np.sqrt(mean_squared_error(pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build pipeline\n",
    "ridge_regression = Pipeline(steps = [\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', Ridge(alpha=3.0))\n",
    "])\n",
    "\n",
    "# fit the model\n",
    "ridge_regression.fit(X_train, y_train)\n",
    "#predict \n",
    "pred_ridge = ridge_regression.predict(X_test)\n",
    "\n",
    "# measure\n",
    "np.sqrt(mean_squared_error(pred_ridge,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build pipeline\n",
    "lasso_regression = Pipeline(steps = [\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', Lasso(alpha=1.0))\n",
    "])\n",
    "\n",
    "# fit model\n",
    "lasso_regression.fit(X_train, y_train)\n",
    "\n",
    "# predict \n",
    "lasso_pred = lasso_regression.predict(X_test)\n",
    "\n",
    "#measure\n",
    "np.sqrt(mean_squared_error(lasso_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline\n",
    "elasticnet_regression = Pipeline(steps = [\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', ElasticNet(alpha=1.0, l1_ratio=0.8))\n",
    "])\n",
    "\n",
    "# fit model\n",
    "elasticnet_regression.fit(X_train, y_train)\n",
    "\n",
    "#predict\n",
    "elasticnet_pred = elasticnet_regression.predict(X_test)\n",
    "\n",
    "#measure\n",
    "np.sqrt(mean_squared_error(elasticnet_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "probability: how likely a specific event is to occur\n",
    "\n",
    "likelilihood: \n",
    "\n",
    "    we are trying to find the best distribution as opposed to already having the distribution available.\n",
    "\n",
    "    how likely we are to observe the given data for different valus of the parameters.  for instance: coin toss, the likelihood helps us estimate the faireness of the coin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " How should you choose the distribution function? How should you choose the parameters for the probability density function?\n",
    " \n",
    " \n",
    " Maximum a Posteriori (MAP) \n",
    "    \n",
    "    MLE assumes that all solutions are equally likely\n",
    "\n",
    "    treats the problem as an optimization problem\n",
    "\n",
    "    We wish to maximize the probability of observing the data from a joint probability distribution given a distribution and parameters that we already know\n",
    " \n",
    "  \n",
    " Maximum Likelihood Estimation (MLE)\n",
    "   \n",
    "    MAP takes into account prior information about the form of the solution\n",
    "\n",
    "    最大似然估计（MLE）的背景下，θ 表示我们试图估计的模型参数。通过最大化给定数据的似然，我们可以找到最佳的参数值 \n",
    "    \n",
    "    When we are fitting a machine learning model, we are essentially trying to estimate probability density - i.e., finding the best parameters or settings that explain the data.\n",
    "\n",
    "    The Maximum Likelihood Estimation (MLE) framework is used for density estimation in both supervised and unsupervised models (and in neural nets) - it provides the basis for both Linear and Logistic Regression models. In summary, you can think of it as an optimization problem of searching a whole bunch of parameters in order to find the BEST parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " log-likelihood function\n",
    " \n",
    "        Multiplying all the small probabilities, however, is time consuming, so we can restate this further as the sum of the log conditional probabilities of observing each example given the model parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结果解读\n",
    "\n",
    "The coefficient for age is 0.07. This means that for each additional year of age (holding other variables constant), the log-odds of purchasing the product increases by 0.07. In other words, the older a person is, the greater the likelihood of purchasing the product."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform statistical significance tests on these coefficient\n",
    "\n",
    "以了解每个预测变量是否真正对响应变量有影响。这通常通过检查系数的 Wald 测试和/或其 p 值来完成。如果 p 值小于某个预定的显著性水平（如0.05），我们通常会认为该变量对预测结果有显著影响。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meansurement - how fit the model is \n",
    "\n",
    "1. Accuracy score\n",
    "\n",
    "    does not work well for imbalanced data\n",
    "\n",
    "    works best for if FP and FN have similar cost.\n",
    "\n",
    "2. precision\n",
    "    \n",
    "    true positive based on all positive preciction.\n",
    "\n",
    "3. Recall = sensitivity (TPR- true positive rate | Sensitivity)\n",
    "\n",
    "    true positive based on all positive actual\n",
    "    \n",
    "4. F1 score\n",
    "\n",
    "    weighted average of precison and recall. \n",
    "\n",
    "    more usuful than accuracy, especially for uneven class distribution. \n",
    "\n",
    "5. Specificity \n",
    "\n",
    "    TN/ all negative\n",
    "\n",
    "6. AUC-ROC \n",
    "    \n",
    "    the area under the ROC curve (AUC-ROC), which represents the model's ability to balance correctly classifying positive instances as positive and negative instances as negative.\n",
    "\n",
    "7. confusion matrix\n",
    "\n",
    "     which shows the number of correctly and incorrectly predicted instances by the model.\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must always remember there will be 'noise' in the data, which is what we typically consider the errors. For logistic regression, we look at MLE to estimate parameters (if you remember, for Linear regression we discussed Least Squares Optimization - this is similar)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.formula.api import logit\n",
    "from statsmodels.graphics.mosaicplot import mosaic\n",
    "\n",
    "model = logit('y~x', data=df).fit()\n",
    "model.params # get two coefficients, intercept and slope\n",
    "\n",
    "#visualizing\n",
    "sns.regplot(x='a', y='b', data=df,ci=None, logistic=True)\n",
    "plt.axline(xy1(0,intercept), slope=slope, color='red')\n",
    "\n",
    "#probability, mostlikely outcome, oddsratio, log oddsratio\n",
    "# get the most likely outcome 0 or 1\n",
    "prediction_data['most_likely_outcome'] = np.round(prediction_data['has_churned'])\n",
    "odds_ratio = probability/1-probability\n",
    "prediction_data['log_odds_ratio'] = np.log(prediction_dta['odds_ratio'])\n",
    "\n",
    "#measurement\n",
    "conf_matrix = model.pred_table()\n",
    "mosaic(conf_matrix) # plot confusion matrix\n",
    "\n",
    "#accuracy: the % of the correct predictions\n",
    "# sensitivity: the % of the true positive , higher is better \n",
    "# specificity: the % of true negative, higher is better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression | GaussianNB | KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, roc_curve, roc_auc_score, precision_recall_curve, confusion_matrix, recall_score\n",
    "\n",
    "\n",
    "# there are many more hyperparameters we can adjust in this version, \n",
    "# such as regularization terms (we'll cover this in a later module)\n",
    "# weights, solver type, etc. for now, we'll keep the default settings\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "log = LogisticRegression()\n",
    "nb = GaussianNB()\n",
    "knn = KNeighborsClassifier() #default neighbours is 5\n",
    "\n",
    "#split the data\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2)\n",
    "\n",
    "log.fit(x_train,y_train)\n",
    "nb.fit(x_train,y_train)\n",
    "knn.fit(x_train,y_train)\n",
    "\n",
    "# 预测生死的可能性百分比\n",
    "yhat_log = log.predict_proba(x_test)\n",
    "yhat_nb = nb.predict_proba(x_test)\n",
    "yhat_knn = knn.predict(x_test) \n",
    "# does have predict probability, \n",
    "# only have predict function.\n",
    "# always check the method on Sklearn website.\n",
    "\n",
    "yhat_log   # two number, probably of 0, and probabily of 1\n",
    "\n",
    "#这个结果只有生或者 死，不是百分比\n",
    "log.predict(x_test) \n",
    "# isolate only the probabilities that something is a 1 (alive). \n",
    "yhat_log[:,1] \n",
    "\n",
    "# meansurement\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, yhat_log[:,1])\n",
    "fpr_nb, tpr_nb, thresholds_nb = roc_curve(y_test, yhat_nb[:,1])\n",
    "\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(fpr,tpr)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(fpr_nb,tpr_nb)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "\n",
    "gmeans = np.sqrt(tpr*(1-fpr))\n",
    "gmeans_nb = np.sqrt(tpr_nb*(1-fpr_nb))\n",
    "\n",
    "\n",
    "#find the index with the highest gmean\n",
    "\n",
    "print(thresholds[np.argmax(gmeans)])\n",
    "print(gmeans[np.argmax(gmeans)])\n",
    "print(thresholds_nb[np.argmax(gmeans_nb)])\n",
    "print(gmeans_nb[np.argmax(gmeans_nb)])\n",
    "\n",
    "\n",
    "\n",
    "# KNN performance\n",
    "fpr_knn, tpr_knn, thresholds_knn = roc_curve(y_test, yhat_knn)\n",
    "plt.plot(fpr_knn,tpr_knn, label='knn')\n",
    "plt.legend()\n",
    "\n",
    "print(\"knn F1: \", f1_score(y_test,yhat_knn))\n",
    "print(\"knn precision: \", precision_score(y_test,yhat_knn))\n",
    "print(\"knn precision: \", recall_score(y_test,yhat_knn))\n",
    "print(\"knn precision: \", accuracy_score(y_test,yhat_knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above. \n",
    "Ideally we want this curve to be towards the top left; but in a non-ideal world, we want to find the optimal threshold. One way we can do this is by calculating the geometric mean (G-mean) which will find the balance between Sensitivity and Specificity. As a refresher:\n",
    "\n",
    "Sensitivity = True Positive Rate \\\n",
    "Specificity = 1 - False Positive Rate\n",
    "\n",
    "In other words:\n",
    "\n",
    "Sensitivity = TP / (TP + FN) \\\n",
    "Specificity = TN / (FP + TN)\n",
    "\n",
    "To calculate the G-mean, we simply take the square root of the Sensitivity multiplied by the Specificity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN & Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_columns = ['Pclass','Sex','Embarked']\n",
    "num_columns = ['Age','SibSp','Parch','Fare']\n",
    "target = 'Survived'\n",
    "\n",
    "\n",
    "# build pipeline\n",
    "# pipeline can do data clearning and variaable preparation\n",
    "cat_transformer = OneHotEncoder(handle_unknown='ignore') # does not have model in it.\n",
    "num_transformer = StandardScaler()\n",
    "preprocessor = ColumnTransformer(transformers = [('cat',cat_transformer, cat_columns),\n",
    "                                                 ('num', num_transformer, num_columns)])\n",
    "\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[cat_columns + num_columns], df[target], test_size=0.2, random_state=42)\n",
    "\n",
    "# transform dataset using pipeline\n",
    "# scaling, feature engineering\n",
    "x_train_transformed = preprocessor.fit_transform(X_train)\n",
    "x_test_transformed = preprocessor.fit_transform(X_test)\n",
    "\n",
    "x_train_transformed.shape\n",
    "x_test_transformed.shape\n",
    "\n",
    "# encoding columns and features\n",
    "encoded_columns = list(preprocessor.named_transformers_['cat'].get_feature_names_out(cat_columns))\n",
    "all_feat = num_columns + encoded_columns\n",
    "\n",
    "X_train_transformed = pd.DataFrame(x_train_transformed, columns=all_feat)\n",
    "X_test_transformed = pd.DataFrame(x_test_transformed, columns=all_feat)\n",
    "\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 40)\n",
    "knn.fit(X_train_transformed, y_train)\n",
    "y_pred = knn.predict(X_test_transformed)\n",
    "print(f1_score(y_pred, y_test))\n",
    "print(accuracy_score(y_pred, y_test))\n",
    "\n",
    "\n",
    "# same syntax with other models\n",
    "recall = []\n",
    "\n",
    "for a in range(1,100):  # loop with values from 1 to 100, every single result depends on the accuracies. \n",
    "        \n",
    "    knn = KNeighborsClassifier(n_neighbors = a)\n",
    "    knn.fit(X_train_transformed, y_train)\n",
    "    y_pred = knn.predict(X_test_transformed)\n",
    "    # print(f1_score(y_pred, y_test))\n",
    "    accuracies.append(accuracy_score(y_pred, y_test))\n",
    "    # check the f1 score, \n",
    "    # check recore score\n",
    "    # check precision score. \n",
    "    recall.append(recall_score(y_pred, y_test))\n",
    "    \n",
    "    \n",
    "plt.plot(recall)\n",
    "\n",
    "plt.plot(accuracies)\n",
    "\n",
    "\n",
    "\n",
    "# Hyperparameter tuning\n",
    "# select parameters\n",
    "params_nb = {'var_smoothing': np.logspace(0,-9, num=100)}\n",
    "params_knn = {'n_neighbors': list(range(1,100)), 'weights': ['uniform', 'distance']}\n",
    "# tested listed neighbours, what all of the options, we only seleted  n_neighbours, range 1-100, \n",
    "\n",
    "# Define the models\n",
    "nb = GaussianNB()\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Define GridSearchCV, we can use either of the search ways. \n",
    "gridsearch_nb = GridSearchCV(nb, params_nb, cv=5, scoring='roc_auc') # search every possible combination, build 200 models, tested all of , and tell you which have the best score. \n",
    "    #nb, params_nb, cv=5). # you can decide which score you want.\n",
    "    # run 200 models, 5 model each.\n",
    "randomsearch_knn = RandomizedSearchCV(knn, params_knn, cv=10, scoring='roc_auc') #  try 10 random combination. \n",
    "randomsearch_knn = RandomizedSearchCV(knn, params_knn, cv=10,n_iter=50, scoring='roc_auc'), # will try 50 possible combination. try 50, total 500\n",
    "#cv=10, cross validation. resample the data 10 times, and run the model each time, return the best parameters based on how well. \n",
    "\n",
    "\n",
    "\n",
    "# Fit models\n",
    "gridsearch_nb.fit(X_train_transformed, y_train)\n",
    "randomsearch_knn.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Get the best parameters\n",
    "print(\"Best parameters for KNN gridsearch: \", gridsearch_knn.best_params_)\n",
    "print(\"Best parameters for KNN: \", randomsearch_knn.best_params_)\n",
    "\n",
    "#选择出了最好的parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN & NAIVE BASE MACHINE LEARNING\n",
    "\n",
    "1. average performance,\n",
    "2. outliers performance \n",
    "3. perfomance should be consistance\n",
    "this is the recall score\n",
    "4. average of the 5 test scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the best parameters\n",
    "nb_best = GaussianNB(**gridsearch_nb.best_params_)\n",
    "knn_best = KNeighborsClassifier(**randomsearch_knn.best_params_)\n",
    "\n",
    "# Perform cross-validation, run as model itself, just see how well the model performance, \n",
    "cv_scores_nb = cross_val_score(nb_best, X_train_transformed, y_train, cv=5, scoring='roc_auc')\n",
    "cv_scores_knn = cross_val_score(knn_best, X_train_transformed, y_train, cv=5, scoring='roc_auc')  \n",
    "\n",
    "print(\"Cross-validation scores for Naive Bayes: \", cv_scores_nb)\n",
    "print(\"Cross-validation scores for KNN: \", cv_scores_knn)\n",
    "\n",
    "# Fit the models with the best parameters\n",
    "nb_best.fit(X_train_transformed, y_train)\n",
    "knn_best.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Evaluate on the test set\n",
    "y_pred_nb = nb_best.predict(X_test_transformed)\n",
    "y_pred_knn = knn_best.predict(X_test_transformed)\n",
    "\n",
    "print(\"Test accuracy for Naive Bayes: \", accuracy_score(y_test, y_pred_nb))\n",
    "print(\"Test accuracy for KNN: \", accuracy_score(y_test, y_pred_knn))\n",
    "\n",
    "\n",
    "\n",
    "# Compute metrics\n",
    "print(\"Naive Bayes Metrics: \")\n",
    "print(\"Confusion Matrix: \", confusion_matrix(y_test, y_pred_nb))\n",
    "print(\"Accuracy: \", accuracy_score(y_test, y_pred_nb))\n",
    "print(\"Precision: \", precision_score(y_test, y_pred_nb))\n",
    "print(\"Recall: \", recall_score(y_test, y_pred_nb))\n",
    "print(\"F1 Score: \", f1_score(y_test, y_pred_nb))\n",
    "\n",
    "print(\"\\nK-Nearest Neighbors Metrics: \")\n",
    "print(\"Confusion Matrix: \", confusion_matrix(y_test, y_pred_knn))\n",
    "print(\"Accuracy: \", accuracy_score(y_test, y_pred_knn))\n",
    "print(\"Precision: \", precision_score(y_test, y_pred_knn))\n",
    "print(\"Recall: \", recall_score(y_test, y_pred_knn))\n",
    "print(\"F1 Score: \", f1_score(y_test, y_pred_knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
